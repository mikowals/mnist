Job launching after 0.11 seconds in submission.
Running python job.
Changed into dir /home/ubuntu/mnist
Importing fully_connected_feed.py
Running fully_connected_feed.main()
I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:888] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 0 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 4.00GiB
Free memory: 3.95GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:644] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 3927400448
I tensorflow/core/common_runtime/direct_session.cc:45] Direct session inter op parallelism threads: 8
Got result 0.1203
Completed successfully in 91.71 seconds. [{u'main': 0.12029999999999996}]
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Step 0: loss = 2.32 (0.142 sec)
Step 100: loss = 2.28 (0.015 sec)
Step 200: loss = 2.25 (0.014 sec)
Step 300: loss = 2.22 (0.014 sec)
Step 400: loss = 2.22 (0.015 sec)
Step 500: loss = 2.21 (0.076 sec)
Step 600: loss = 2.19 (0.014 sec)
Step 700: loss = 2.14 (0.014 sec)
Step 800: loss = 2.13 (0.014 sec)
Step 900: loss = 2.06 (0.014 sec)
training Data Eval:
Test Data Eval:
  Num examples: 10000  Num correct: 7406  Precision @ 1: 0.7406
Step 1000: loss = 1.96 (0.071 sec)
Step 1100: loss = 1.97 (0.014 sec)
Step 1200: loss = 1.95 (0.014 sec)
Step 1300: loss = 1.82 (0.014 sec)
Step 1400: loss = 1.74 (0.014 sec)
Step 1500: loss = 1.75 (0.075 sec)
Step 1600: loss = 1.63 (0.014 sec)
Step 1700: loss = 1.58 (0.014 sec)
Step 1800: loss = 1.58 (0.014 sec)
Step 1900: loss = 1.46 (0.014 sec)
training Data Eval:
Test Data Eval:
  Num examples: 10000  Num correct: 7817  Precision @ 1: 0.7817
Step 2000: loss = 1.39 (0.069 sec)
Step 2100: loss = 1.39 (0.015 sec)
Step 2200: loss = 1.30 (0.014 sec)
Step 2300: loss = 1.29 (0.014 sec)
Step 2400: loss = 1.20 (0.014 sec)
Step 2500: loss = 1.14 (0.076 sec)
Step 2600: loss = 1.01 (0.014 sec)
Step 2700: loss = 1.01 (0.015 sec)
Step 2800: loss = 1.03 (0.014 sec)
Step 2900: loss = 0.89 (0.014 sec)
training Data Eval:
Test Data Eval:
  Num examples: 10000  Num correct: 8200  Precision @ 1: 0.8200
Step 3000: loss = 0.92 (0.076 sec)
Step 3100: loss = 0.89 (0.014 sec)
Step 3200: loss = 0.89 (0.014 sec)
Step 3300: loss = 0.97 (0.014 sec)
Step 3400: loss = 0.81 (0.014 sec)
Step 3500: loss = 0.79 (0.076 sec)
Step 3600: loss = 0.89 (0.014 sec)
Step 3700: loss = 0.76 (0.013 sec)
Step 3800: loss = 0.81 (0.013 sec)
Step 3900: loss = 0.74 (0.015 sec)
training Data Eval:
Test Data Eval:
  Num examples: 10000  Num correct: 8511  Precision @ 1: 0.8511
Step 4000: loss = 0.82 (0.076 sec)
Step 4100: loss = 0.80 (0.014 sec)
Step 4200: loss = 0.62 (0.014 sec)
Step 4300: loss = 0.76 (0.015 sec)
Step 4400: loss = 0.84 (0.015 sec)
Step 4500: loss = 0.57 (0.077 sec)
Step 4600: loss = 0.70 (0.015 sec)
Step 4700: loss = 0.70 (0.014 sec)
Step 4800: loss = 0.69 (0.014 sec)
Step 4900: loss = 0.62 (0.014 sec)
training Data Eval:
Test Data Eval:
  Num examples: 10000  Num correct: 8658  Precision @ 1: 0.8658
Step 5000: loss = 0.67 (0.076 sec)
Step 5100: loss = 0.66 (0.014 sec)
Step 5200: loss = 0.56 (0.013 sec)
Step 5300: loss = 0.58 (0.013 sec)
Step 5400: loss = 0.64 (0.014 sec)
Step 5500: loss = 0.56 (0.076 sec)
Step 5600: loss = 0.59 (0.014 sec)
Step 5700: loss = 0.64 (0.013 sec)
Step 5800: loss = 0.45 (0.015 sec)
Step 5900: loss = 0.72 (0.014 sec)
training Data Eval:
Test Data Eval:
  Num examples: 10000  Num correct: 8797  Precision @ 1: 0.8797
